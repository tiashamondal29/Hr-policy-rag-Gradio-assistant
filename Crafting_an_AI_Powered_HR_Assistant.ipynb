{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOuwdQjcZm9FoocQnauBntp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tiashamondal29/Hr-policy-rag-Gradio-assistant/blob/main/Crafting_an_AI_Powered_HR_Assistant.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "APsOfxcun59K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "372a87e4-1c19-4f5a-86d1-ed6b12e6cdd7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
          ]
        }
      ],
      "source": [
        "# ==============================\n",
        "# STEP 1: Install Required Libraries\n",
        "# ==============================\n",
        "!pip install -q chromadb langchain pypdf gradio langchain-community\n",
        "!pip install -q google-generativeai langchain-google-genai\n",
        "!pip install -q sentence-transformers langchain-text-splitters\n",
        "\n",
        "# ==============================\n",
        "# STEP 2: Import Libraries\n",
        "# ==============================\n",
        "import os\n",
        "from datetime import datetime\n",
        "import gradio as gr\n",
        "\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
        "\n",
        "# ==============================\n",
        "# STEP 3: Setup Google Gemini API Key\n",
        "# ==============================\n",
        "from google.colab import userdata\n",
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.get(\"GOOGLE_API_KEY\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================\n",
        "# STEP 4: Initialize Components\n",
        "# ==============================\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-lite\", temperature=0)\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "\n",
        "# Create directory for vector database\n",
        "PERSIST_DIR = \"/content/vector_db\"\n",
        "os.makedirs(PERSIST_DIR, exist_ok=True)\n",
        "\n",
        "vectorstore = None\n",
        "retriever = None\n",
        "rag_chain = None     # replaces qa_chain\n",
        "feedback_log = []\n",
        "\n",
        "# Helper: format retrieved docs into a single context string\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n---\\n\\n\".join([d.page_content for d in docs])\n",
        "\n",
        "# ==============================\n",
        "# STEP 5: Chatbot Function\n",
        "# ==============================\n",
        "def chatbot(message, history):\n",
        "    try:\n",
        "        global rag_chain\n",
        "        if rag_chain is None:\n",
        "            return \"‚ö†Ô∏è Please upload a PDF document first using the 'Upload Document' tab.\"\n",
        "\n",
        "        # LCEL: invoke the RAG chain with the question\n",
        "        response = rag_chain.invoke(message)\n",
        "\n",
        "        # ChatGoogleGenerativeAI returns an AIMessage with .content\n",
        "        return response.content if hasattr(response, \"content\") else str(response)\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"‚ùå Error: {str(e)}\"\n",
        "\n",
        "# ==============================\n",
        "# STEP 6: Upload PDF Function\n",
        "# ==============================\n",
        "def process_new_pdf(file):\n",
        "    try:\n",
        "        if file is None:\n",
        "            return \"‚ö†Ô∏è Please upload a PDF file first.\"\n",
        "\n",
        "        # Load PDF\n",
        "        loader = PyPDFLoader(file)\n",
        "        documents = loader.load()\n",
        "\n",
        "        # Split into chunks\n",
        "        docs = splitter.split_documents(documents)\n",
        "\n",
        "        # Create vector database\n",
        "        global vectorstore, retriever, rag_chain\n",
        "        vectorstore = Chroma.from_documents(\n",
        "            docs,\n",
        "            embeddings,\n",
        "            collection_name=\"hr_policy_hf_embeddings\",\n",
        "            persist_directory=PERSIST_DIR\n",
        "        )\n",
        "        vectorstore.persist()\n",
        "\n",
        "        # Setup retriever\n",
        "        retriever = vectorstore.as_retriever()\n",
        "\n",
        "        # Define RAG prompt\n",
        "        prompt = ChatPromptTemplate.from_template(\n",
        "            \"\"\"\n",
        "You are a helpful HR assistant. Use ONLY the following HR policy context to answer.\n",
        "If the answer is not clearly present, say you don't know.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "Answer in very simple language so that any employee can understand.\n",
        "\"\"\"\n",
        "        )\n",
        "\n",
        "\n",
        "        rag_chain = (\n",
        "            RunnableParallel(\n",
        "                context=retriever | format_docs,\n",
        "                question=RunnablePassthrough(),\n",
        "            )\n",
        "            | prompt\n",
        "            | llm\n",
        "        )\n",
        "\n",
        "        return f\"\"\"‚úÖ Successfully processed!\n",
        "üìÑ Total chunks: {len(docs)}\n",
        "üìÇ Saved to: {PERSIST_DIR}\n",
        "\n",
        "üëâ Go to 'Chat' tab to ask questions!\"\"\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"‚ùå Error: {str(e)}\"\n",
        "\n",
        "# ==============================\n",
        "# STEP 7: Feedback Function\n",
        "# ==============================\n",
        "def save_feedback(feedback_text):\n",
        "    if not feedback_text or feedback_text.strip() == \"\":\n",
        "        return \"‚ö†Ô∏è Please enter feedback before submitting.\"\n",
        "\n",
        "    feedback_log.append({\n",
        "        \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "        \"feedback\": feedback_text\n",
        "    })\n",
        "    return \"‚úÖ Thank you for your feedback!\""
      ],
      "metadata": {
        "id": "Q-t4UMm9p7Em",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9656e5bf-fa61-406b-be22-5bf5e471a22e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-115054707.py:4: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 252b5ea5-5d00-442b-aa71-8404282a36b6)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/./sentence_bert_config.json\n",
            "WARNING:huggingface_hub.utils._http:'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 252b5ea5-5d00-442b-aa71-8404282a36b6)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/./sentence_bert_config.json\n",
            "Retrying in 1s [Retry 1/5].\n",
            "WARNING:huggingface_hub.utils._http:Retrying in 1s [Retry 1/5].\n",
            "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 5f8cf787-5fed-4308-8ee5-d54c22ebe9aa)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json\n",
            "WARNING:huggingface_hub.utils._http:'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 5f8cf787-5fed-4308-8ee5-d54c22ebe9aa)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json\n",
            "Retrying in 1s [Retry 1/5].\n",
            "WARNING:huggingface_hub.utils._http:Retrying in 1s [Retry 1/5].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================\n",
        "# STEP 8: Create Gradio Interface (Updated UI)\n",
        "# ==============================\n",
        "import gradio as gr\n",
        "\n",
        "APP_CSS = \"\"\"\n",
        ":root { --radius: 14px; }\n",
        "\n",
        "#app-header {\n",
        "  padding: 14px 18px;\n",
        "  border-radius: var(--radius);\n",
        "  border: 1px solid rgba(255,255,255,0.08);\n",
        "  background: linear-gradient(135deg, rgba(255,255,255,0.06), rgba(255,255,255,0.02));\n",
        "  margin-bottom: 14px;\n",
        "}\n",
        "\n",
        ".badge {\n",
        "  display: inline-flex;\n",
        "  gap: 8px;\n",
        "  align-items: center;\n",
        "  padding: 6px 10px;\n",
        "  border-radius: 999px;\n",
        "  border: 1px solid rgba(255,255,255,0.10);\n",
        "  background: rgba(255,255,255,0.04);\n",
        "  font-size: 12px;\n",
        "  margin-right: 8px;\n",
        "}\n",
        "\n",
        ".kpanel {\n",
        "  border-radius: var(--radius);\n",
        "  border: 1px solid rgba(255,255,255,0.10);\n",
        "  background: rgba(255,255,255,0.03);\n",
        "  padding: 14px;\n",
        "}\n",
        "\n",
        ".small {\n",
        "  font-size: 12px;\n",
        "  opacity: 0.9;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "def get_system_status():\n",
        "    \"\"\"Small helper to display backend readiness in the UI (no change to RAG logic).\"\"\"\n",
        "    ready = rag_chain is not None\n",
        "    db = \"Ready ‚úÖ\" if ready else \"Not ready ‚ùå\"\n",
        "    chunks = \"Loaded ‚úÖ\" if ready else \"‚Äî\"\n",
        "    return f\"{db}\", f\"{chunks}\"\n",
        "\n",
        "def set_example(q):\n",
        "    \"\"\"Convenience: put suggested question into textbox.\"\"\"\n",
        "    return q\n",
        "\n",
        "with gr.Blocks(theme=gr.themes.Soft(), title=\"AI HR Assistant\", css=APP_CSS) as demo:\n",
        "\n",
        "    # --- Header ---\n",
        "    gr.Markdown(\n",
        "        \"\"\"\n",
        "<div id=\"app-header\">\n",
        "  <div style=\"display:flex;justify-content:space-between;align-items:flex-start;gap:12px;flex-wrap:wrap;\">\n",
        "    <div>\n",
        "      <h2 style=\"margin:0;\">ü§ñ Nestl√© HR Policy Assistant</h2>\n",
        "      <div class=\"small\">Ask questions about HR policies and get answers grounded in the uploaded PDF.</div>\n",
        "    </div>\n",
        "    <div style=\"text-align:right;\">\n",
        "      <span class=\"badge\">üîé RAG + Vector Search</span>\n",
        "      <span class=\"badge\">üìÑ PDF-grounded</span>\n",
        "      <span class=\"badge\">üß© Gradio UI</span>\n",
        "    </div>\n",
        "  </div>\n",
        "</div>\n",
        "        \"\"\",\n",
        "    )\n",
        "\n",
        "    with gr.Tabs():\n",
        "\n",
        "        # =======================\n",
        "        # TAB 1: CHAT (Revamped)\n",
        "        # =======================\n",
        "        with gr.Tab(\"üí¨ Chat\"):\n",
        "            with gr.Row(equal_height=True):\n",
        "                # Left: Chat\n",
        "                with gr.Column(scale=7):\n",
        "                    gr.Markdown(\"### Chat with your HR Assistant\")\n",
        "                    chat = gr.Chatbot(label=\"Conversation\", height=430)\n",
        "\n",
        "                    with gr.Row():\n",
        "                        msg = gr.Textbox(\n",
        "                            label=\"Type your question\",\n",
        "                            placeholder=\"e.g., What is the annual leave policy?\",\n",
        "                            lines=2\n",
        "                        )\n",
        "                    with gr.Row():\n",
        "                        send = gr.Button(\"Send ‚û§\", variant=\"primary\")\n",
        "                        clear = gr.Button(\"Clear\", variant=\"secondary\")\n",
        "\n",
        "                    # Use the same backend chatbot(message, history)\n",
        "                    def chat_submit(user_message, history):\n",
        "                        # history is list[tuple[user, bot]]\n",
        "                        bot_reply = chatbot(user_message, history)\n",
        "                        history = history + [(user_message, bot_reply)]\n",
        "                        return history, \"\"\n",
        "\n",
        "                    send.click(fn=chat_submit, inputs=[msg, chat], outputs=[chat, msg])\n",
        "                    msg.submit(fn=chat_submit, inputs=[msg, chat], outputs=[chat, msg])\n",
        "\n",
        "                    clear.click(lambda: [], outputs=chat)\n",
        "\n",
        "                # Right: Knowledge Panel\n",
        "                with gr.Column(scale=4):\n",
        "                    gr.Markdown(\"### Knowledge Panel\")\n",
        "                    with gr.Group(elem_classes=[\"kpanel\"]):\n",
        "                        gr.Markdown(\n",
        "                            \"\"\"\n",
        "**Quick tips**\n",
        "- Upload the HR policy PDF in the **Upload Document** tab first.\n",
        "- Ask short, specific questions for best results.\n",
        "- If the answer isn't in the document, the bot should say *I don't know*.\n",
        "                            \"\"\"\n",
        "                        )\n",
        "\n",
        "                        db_status = gr.Textbox(label=\"Vector DB\", interactive=False)\n",
        "                        chunk_status = gr.Textbox(label=\"Document Chunks\", interactive=False)\n",
        "                        refresh = gr.Button(\"üîÑ Refresh status\")\n",
        "\n",
        "                        refresh.click(fn=get_system_status, inputs=[], outputs=[db_status, chunk_status])\n",
        "\n",
        "                        gr.Markdown(\"**Suggested questions**\")\n",
        "                        ex1 = gr.Button(\"What is the leave policy?\")\n",
        "                        ex2 = gr.Button(\"How do I request time off?\")\n",
        "                        ex3 = gr.Button(\"What are employee benefits?\")\n",
        "                        ex4 = gr.Button(\"What is the work from home policy?\")\n",
        "                        ex5 = gr.Button(\"How do I submit expense claims?\")\n",
        "\n",
        "                        ex1.click(fn=set_example, inputs=[], outputs=msg, api_name=False).then(None, None, None)\n",
        "                        ex2.click(fn=set_example, inputs=[], outputs=msg, api_name=False).then(None, None, None)\n",
        "                        ex3.click(fn=set_example, inputs=[], outputs=msg, api_name=False).then(None, None, None)\n",
        "                        ex4.click(fn=set_example, inputs=[], outputs=msg, api_name=False).then(None, None, None)\n",
        "                        ex5.click(fn=set_example, inputs=[], outputs=msg, api_name=False).then(None, None, None)\n",
        "\n",
        "            gr.Markdown(\n",
        "                \"<div class='small'>Note: Answers are generated from the uploaded policy document. If you upload a new PDF, refresh status.</div>\"\n",
        "            )\n",
        "\n",
        "        # =======================\n",
        "        # TAB 2: UPLOAD DOCUMENT\n",
        "        # =======================\n",
        "        with gr.Tab(\"üì§ Upload Document\"):\n",
        "            gr.Markdown(\"### Upload and index your HR policy PDF\")\n",
        "\n",
        "            with gr.Row():\n",
        "                with gr.Column(scale=6):\n",
        "                    file_input = gr.File(\n",
        "                        label=\"Upload HR Policy PDF\",\n",
        "                        file_types=[\".pdf\"],\n",
        "                        type=\"filepath\"\n",
        "                    )\n",
        "                    upload_btn = gr.Button(\"üîÑ Process Document\", variant=\"primary\")\n",
        "\n",
        "                with gr.Column(scale=6):\n",
        "                    status_output = gr.Textbox(label=\"Processing Status\", lines=10, interactive=False)\n",
        "\n",
        "            upload_btn.click(\n",
        "                fn=process_new_pdf,\n",
        "                inputs=[file_input],\n",
        "                outputs=[status_output]\n",
        "            )\n",
        "\n",
        "            gr.Markdown(\n",
        "                \"\"\"\n",
        "**How it works**\n",
        "1. Upload PDF\n",
        "2. We split it into chunks\n",
        "3. Create embeddings + store in Chroma\n",
        "4. Chat tab will retrieve relevant chunks and answer\n",
        "                \"\"\"\n",
        "            )\n",
        "\n",
        "        # =======================\n",
        "        # TAB 3: FEEDBACK\n",
        "        # =======================\n",
        "        with gr.Tab(\"üí≠ Feedback\"):\n",
        "            gr.Markdown(\"### Help us improve\")\n",
        "\n",
        "            feedback_text = gr.Textbox(\n",
        "                label=\"Your feedback\",\n",
        "                lines=6,\n",
        "                placeholder=\"What worked well? What felt wrong or missing?\"\n",
        "            )\n",
        "            with gr.Row():\n",
        "                feedback_btn = gr.Button(\"üì® Submit Feedback\", variant=\"primary\")\n",
        "                feedback_clear = gr.Button(\"Clear\")\n",
        "\n",
        "            feedback_status = gr.Textbox(label=\"Status\", interactive=False)\n",
        "\n",
        "            feedback_btn.click(\n",
        "                fn=save_feedback,\n",
        "                inputs=[feedback_text],\n",
        "                outputs=[feedback_status]\n",
        "            )\n",
        "            feedback_clear.click(lambda: \"\", outputs=feedback_text)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-gMgCVvZqlWl",
        "outputId": "f933242a-914f-4fd3-b2f8-f9cc3e9e5cd9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3853409123.py:53: DeprecationWarning: The 'theme' parameter in the Blocks constructor will be removed in Gradio 6.0. You will need to pass 'theme' to Blocks.launch() instead.\n",
            "  with gr.Blocks(theme=gr.themes.Soft(), title=\"AI HR Assistant\", css=APP_CSS) as demo:\n",
            "/tmp/ipython-input-3853409123.py:53: DeprecationWarning: The 'css' parameter in the Blocks constructor will be removed in Gradio 6.0. You will need to pass 'css' to Blocks.launch() instead.\n",
            "  with gr.Blocks(theme=gr.themes.Soft(), title=\"AI HR Assistant\", css=APP_CSS) as demo:\n",
            "/tmp/ipython-input-3853409123.py:84: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chat = gr.Chatbot(label=\"Conversation\", height=430)\n",
            "/tmp/ipython-input-3853409123.py:84: DeprecationWarning: The default value of 'allow_tags' in gr.Chatbot will be changed from False to True in Gradio 6.0. You will need to explicitly set allow_tags=False if you want to disable tags in your chatbot.\n",
            "  chat = gr.Chatbot(label=\"Conversation\", height=430)\n",
            "/tmp/ipython-input-3853409123.py:134: DeprecationWarning: Setting 'api_name=False' in event listeners will be removed in Gradio 6.0. You will need to use 'api_visibility=\"private\"' instead.\n",
            "  ex1.click(fn=set_example, inputs=[], outputs=msg, api_name=False).then(None, None, None)\n",
            "/usr/local/lib/python3.12/dist-packages/gradio/utils.py:1052: UserWarning: Expected 1 arguments for function <function set_example at 0x7c964082f920>, received 0.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/gradio/utils.py:1056: UserWarning: Expected at least 1 arguments for function <function set_example at 0x7c964082f920>, received 0.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-3853409123.py:135: DeprecationWarning: Setting 'api_name=False' in event listeners will be removed in Gradio 6.0. You will need to use 'api_visibility=\"private\"' instead.\n",
            "  ex2.click(fn=set_example, inputs=[], outputs=msg, api_name=False).then(None, None, None)\n",
            "/tmp/ipython-input-3853409123.py:136: DeprecationWarning: Setting 'api_name=False' in event listeners will be removed in Gradio 6.0. You will need to use 'api_visibility=\"private\"' instead.\n",
            "  ex3.click(fn=set_example, inputs=[], outputs=msg, api_name=False).then(None, None, None)\n",
            "/tmp/ipython-input-3853409123.py:137: DeprecationWarning: Setting 'api_name=False' in event listeners will be removed in Gradio 6.0. You will need to use 'api_visibility=\"private\"' instead.\n",
            "  ex4.click(fn=set_example, inputs=[], outputs=msg, api_name=False).then(None, None, None)\n",
            "/tmp/ipython-input-3853409123.py:138: DeprecationWarning: Setting 'api_name=False' in event listeners will be removed in Gradio 6.0. You will need to use 'api_visibility=\"private\"' instead.\n",
            "  ex5.click(fn=set_example, inputs=[], outputs=msg, api_name=False).then(None, None, None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "gr.close_all()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xra-fQfbW8Bg",
        "outputId": "b66f195b-6a24-4213-e385-56126595b892"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Closing server running on port: 7860\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "port = int(os.environ.get(\"PORT\", 7860))\n",
        "host = \"0.0.0.0\"\n",
        "print(f\"üöÄ Starting Gradio on {host}:{port}\")\n",
        "\n",
        "demo.launch(\n",
        "    server_name=host,\n",
        "    server_port=port,\n",
        "    share=True,\n",
        ")\n"
      ],
      "metadata": {
        "id": "N4a6LDyc_hJk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 608
        },
        "outputId": "1c349c02-3848-4436-d5fe-f79cb887e277"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Starting Gradio on 0.0.0.0:7860\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://eacc9b99458b1cdb50.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://eacc9b99458b1cdb50.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "XPGCHR0XQlpo"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8dfYo_-TUm3s"
      },
      "execution_count": 5,
      "outputs": []
    }
  ]
}